---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

deeplearning 
=====

#### Create and train deep neural network of ReLU type with SGD and batch normalization

### About
The deeplearning package is an R package that implements deep neural networks in R. It employes Rectifier Linear Unit functions as its building blocks and trains a neural network with stochastic gradient descent method with batch normalization to speed up the training and promote regularization. Neural networks of such kind of architecture and training methods are state of the art and even achieved suplassing human-level performance in ImageNet competition. The deeplearning package is inspired by another R package darch which implements layerwise Restricted Boltzmann Machine pretraining and dropout and uses its class DArch as the default class. 

### Installtion 

Install deeplearning from CRAN 
```
install.packages("deeplearning")
```

Or install it from github 
```
devtools::install_github("rz1988/deeplearning")

```

### Use deeplearning 

Using the deeplearning package is designed to be easy and fun. It only takes two steps to run your first neural network. 

In step one, the user will create a new neural network. You will need to specify the strucutre of the neural network which are the number of layers and neurons in the network and the type of activation functions. The default activation is rectifier linear unit function for the hidden layers but you can also use other types of activation such as sigmoidal function or write your own activation function.

In step two, the user will train the neural network with a training input and a traing target. There are a number of other training parameters. For how to choose these training parameters please refer to https://github.com/rz1988/deeplearning. 

### Examples 

#### Train a neural networ for regression 

```
input <- matrix(runif(1000), 500, 2)
input_valid <- matrix(runif(100), 50, 2)
target <- rowSums(input + input^2)
target_valid <- rowSums(input_valid + input_valid^2)


# create a new deep neural network for classificaiton
dnn_regression <- new_dnn(
                          c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.
                                                # The first element is the number of input variables.
                                                # The last element is the number of output variables.
                          hidden_layer_default = rectified_linear_unit_function, 
                          # for hidden layers, use rectified_linear_unit_function
                          output_layer_default = linearUnitDerivative # for regression, use linearUnitDerivative function
                          )

dnn_regression <- train_dnn(
                     dnn_regression,

                     # training data
                     input, # input variable for training
                     target, # target variable for training
                     input_valid, # input variable for validation
                     target_valid, # target variable for validation

                     # training parameters
                     learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout
                     learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout
                     learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used
                     batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence
                     batch_normalization = T, # logical value, T to use batch normalization
                     dropout_input = 0.2, # dropout ratio in input.
                     dropout_hidden = 0.5, # dropout ratio in hidden layers
                     momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training
                     momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training
                     momentum_switch = 100, # after which the momentum is switched from initial to final momentum
                     num_epochs = 300, # number of iterations in training

                     # Error function
                     error_function = meanSquareErr, # error function to minimize during training. For regression, use meanSquareErr
                     report_classification_error = F # whether to print classification error during training
)

# the prediciton by dnn_regression
pred <- predict(dnn_regression)

# calculate the r-squared of the prediciton
rsq(dnn_regression)

# calcualte the r-squared of the prediciton in validation
rsq(dnn_regression, input = input_valid, target = target_valid)
```

#### Train a neural network for classification 

```

input <- matrix(runif(1000), 500, 2)
input_valid <- matrix(runif(100), 50, 2)
target <- (cos(rowSums(input + input^2)) > 0.5) * 1
target_valid <- (cos(rowSums(input_valid + input_valid^2)) > 0.5) * 1

# create a new deep neural network for classificaiton
dnn_classification <- new_dnn(
  c(2, 50, 50, 20, 1),  # The layer structure of the deep neural network.
                        # The first element is the number of input variables.
                        # The last element is the number of output variables.
  hidden_layer_default = rectified_linear_unit_function, # for hidden layers, use rectified_linear_unit_function
  output_layer_default = sigmoidUnitDerivative # for classification, use sigmoidUnitDerivative function
)

dnn_classification <- train_dnn(
  dnn_classification,

  # training data
  input, # input variable for training
  target, # target variable for training
  input_valid, # input variable for validation
  target_valid, # target variable for validation

  # training parameters
  learn_rate_weight = exp(-8) * 10, # learning rate for weights, higher if use dropout
  learn_rate_bias = exp(-8) * 10, # learning rate for biases, hihger if use dropout
  learn_rate_gamma = exp(-8) * 10, # learning rate for the gamma factor used
  batch_size = 10, # number of observations in a batch during training. Higher for faster training. Lower for faster convergence
  batch_normalization = T, # logical value, T to use batch normalization
  dropout_input = 0.2, # dropout ratio in input.
  dropout_hidden = 0.5, # dropout ratio in hidden layers
  momentum_initial = 0.6, # initial momentum in Stochastic Gradient Descent training
  momentum_final = 0.9, # final momentum in Stochastic Gradient Descent training
  momentum_switch = 100, # after which the momentum is switched from initial to final momentum
  num_epochs = 100, # number of iterations in training

  # Error function
  error_function = crossEntropyErr, # error function to minimize during training. For regression, use crossEntropyErr
  report_classification_error = T # whether to print classification error during training
)

# the prediciton by dnn_regression
pred <- predict(dnn_classification)

hist(pred)

# calculate the r-squared of the prediciton
AR(dnn_classification)

# calcualte the r-squared of the prediciton in validation
AR(dnn_classification, input = input_valid, target = target_valid)

# print the layer weights
# this function can print heatmap, histogram, or a surface
print_weight(dnn_regression, 1, type = "heatmap")

print_weight(dnn_regression, 2, type = "surface")

print_weight(dnn_regression, 3, type = "histogram")
```

#### References 
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 2013, Dropout: A Simple Way to Prevent Neural Networks from Overfitting, Journal of Machine Learning Research 15 (2014) 1929-1958

Sergey Ioffe, Christian Szegedy, 2015, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv

X. Glorot, A. Bordes, and Y. Bengio, 2011,Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, pages 315â€“323


Drees, Martin (2013). "Implementierung und Analyse von tiefen Architekturen
in R". German. Master's thesis. Fachhochschule Dortmund.

Rueckert, Johannes (2015). "Extending the Darch library for deep
architectures". Project thesis. Fachhochschule Dortmund.
URL: [saviola.de](http://static.saviola.de/publications/rueckert_2015.pdf)


 
